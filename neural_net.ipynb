{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a36e96",
   "metadata": {
    "id": "03347064"
   },
   "source": [
    "Intro for all models; this'll be the same for everyone; post any changes so that everyone can do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d58cd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.1-py2.py3-none-any.whl (249 kB)\n",
      "Collecting et-xmlfile\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\tweek\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b46da549",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e6bf10",
   "metadata": {
    "id": "0b0543d8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score,classification_report,f1_score,roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7476fbb4",
   "metadata": {
    "id": "420fb765"
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel('ModelingDataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc74927",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b6c0765",
    "outputId": "c326947f-6a81-4fc7-e446-9aee8f2e21e3"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'term_category'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\users\\tweek\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\users\\tweek\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\users\\tweek\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'term_category'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#this should've been done in the data cleaning step, Nehal can you add this to our later reports/ppt as part of data cleaning?\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df\u001b[38;5;241m=\u001b[39mdf[((\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mterm_category\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misna())\u001b[38;5;241m&\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memployee_status\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActive\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m|\u001b[39m((df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm_category\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVoluntary Termination\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m&\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memployee_status\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTerminated\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#output var creation\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memployee_status\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memployee_status\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTerminated\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\tweek\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\users\\tweek\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'term_category'"
     ]
    }
   ],
   "source": [
    "#this should've been done in the data cleaning step, Nehal can you add this to our later reports/ppt as part of data cleaning?\n",
    "df=df[((df['term_category'].isna())&(df['employee_status']=='Active'))|((df['term_category']=='Voluntary Termination')&(df['employee_status']=='Terminated'))]\n",
    "#output var creation\n",
    "df['employee_status']=df['employee_status'].apply(lambda x: 1 if x=='Terminated' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b0e5e",
   "metadata": {
    "id": "0182297d"
   },
   "outputs": [],
   "source": [
    "#these vars can directly predict output variable; need to drop them for sure\n",
    "#work structure has a very high correlation with the output, hence dropped. base_pay_mid_point has high correlation with Salary, hence dropped\n",
    "df.drop(['term_category','termination_reason','termination_date','last_promotion_date','work_structure','base_pay_mid_point'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa8002",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33057906",
    "outputId": "8a05a1f2-b48a-4d36-9309-69548c9c3000"
   },
   "outputs": [],
   "source": [
    "#Changing date columns to years\n",
    "#Should've been in data cleaning too \n",
    "\n",
    "for d in [col for col in df.columns if 'date' in col]:\n",
    "    df[d]=df[d].apply(lambda x: str(x.year))\n",
    "\n",
    "#Binning data based on quartiles\n",
    "#dropping old columns here\n",
    "for col in [col for col in df.columns if 'date' in col]:\n",
    "    print(col)\n",
    "    # Fill null values\n",
    "    df[col].fillna('NA', inplace=True)\n",
    "\n",
    "    # Get value counts of each category\n",
    "    value_counts = df[col].value_counts()\n",
    "\n",
    "    # Define bins based on quartiles\n",
    "    bins = [0, value_counts.quantile(0.25), value_counts.quantile(0.5), value_counts.quantile(0.75), np.inf]\n",
    "    labels = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "\n",
    "    # Creating new variable name for the binned data\n",
    "    new_var = col + '_q_binned'\n",
    "\n",
    "    # Bin categories based on quartiles\n",
    "    df[new_var] = pd.cut(df[col].map(value_counts), bins=bins, labels=labels, include_lowest=True)\n",
    "    \n",
    "    #dropping old date columns\n",
    "    #TODO - need to map quartiles to old years to understand \n",
    "    df.drop(col,axis=1,inplace=True)\n",
    "    print(new_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a2ae3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3af825b7",
    "outputId": "e0c88f27-20bc-4f83-c532-dbcc3bdd4cc8"
   },
   "outputs": [],
   "source": [
    "# More data cleaning steps; need to add as having been done before in data cleaning report\n",
    "df['days_since_promotion'][df['days_since_promotion'].isna()]=-1\n",
    "df['marital_status'][df['marital_status'].isna()]='NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba3b56",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bcd8576",
    "outputId": "47a6852d-19a2-489b-81e8-ba2b07e6d899"
   },
   "outputs": [],
   "source": [
    "#Creating dummies and replacing them in the original data\n",
    "\n",
    "\n",
    "num_cols=[]\n",
    "cat_cols=[]\n",
    "for x in df.columns:  \n",
    "    if df[x].dtype=='object':\n",
    "        cat_cols.append(x)\n",
    "    elif len(set(df[x]))<10:\n",
    "        cat_cols.append(x)\n",
    "    else:\n",
    "        num_cols.append(x)\n",
    "        \n",
    "print(num_cols)\n",
    "print(cat_cols)\n",
    "\n",
    "\n",
    "#Getting dummies for variables, drop old ones\n",
    "for var in cat_cols:\n",
    "    if var not in ['employee_status','cost_to_replace_factor']:\n",
    "        dummies = pd.get_dummies(df[var], prefix=var, drop_first=True)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        df.drop(var,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc72325",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0c7da56",
    "outputId": "2d145e24-4488-46e4-93ec-bb18624dd13a"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9c173",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXPkF5UalZJ0",
    "outputId": "b34e2778-df9b-4110-a9ff-94bcd42ce468"
   },
   "outputs": [],
   "source": [
    "# calculate correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# display correlation matrix with just the correlations for the target variable\n",
    "# sort correlation matrix by correlations with the target variable, in descending order\n",
    "corr_matrix_sorted = corr_matrix['employee_status'].sort_values(ascending=False)\n",
    "\n",
    "# display sorted correlation matrix\n",
    "print(corr_matrix_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8bc14b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 905
    },
    "id": "sUV6i7yLlngo",
    "outputId": "2e43fe29-ee40-4fb6-ba0e-169198004a7d"
   },
   "outputs": [],
   "source": [
    "# Set style and color palette\n",
    "sns.set(style=\"white\")\n",
    "sns.set_palette(\"coolwarm\")\n",
    "\n",
    "# Create heatmap with correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "# plot correlation matrix as a heatmap\n",
    "sns.heatmap(corr_matrix, cmap=\"coolwarm\")\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78192239",
   "metadata": {
    "id": "dGVl2IQ1lwGg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.to_excel('ModelingDataset.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f24c358",
   "metadata": {
    "id": "04e63401"
   },
   "source": [
    "Modelling (use your code here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35181e4e",
   "metadata": {
    "id": "d87e7bdb"
   },
   "outputs": [],
   "source": [
    "# set random seed\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc465aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54f7ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('ModelingDataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "218a8b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20321, 73)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9711e591",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dcd8b65",
    "outputId": "5e097fcc-a00f-4d82-834a-3897217a3b02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of binary variable in train set:  0.27165354330708663\n",
      "Ratio of binary variable in test set:  0.2716089880268985\n"
     ]
    }
   ],
   "source": [
    "# separate features and target variable\n",
    "X = df.drop('employee_status', axis=1)\n",
    "y = df['employee_status']\n",
    "\n",
    "# split data into train and test sets, stratifying by the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed, stratify=y)\n",
    "\n",
    "# check the ratio of binary variable in train and test set\n",
    "print(\"Ratio of binary variable in train set: \", y_train.sum() / len(y_train))\n",
    "print(\"Ratio of binary variable in test set: \", y_test.sum() / len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac1c9b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = ['employee_status'] \n",
    "predictors = list(set(list(df.columns))-set(target_column))\n",
    "df[predictors] = df[predictors]/df[predictors].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f617af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14224, 72)\n",
      "(6097, 72)\n"
     ]
    }
   ],
   "source": [
    "X = df[predictors].values\n",
    "y = df[target_column].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=random_seed, stratify = y)\n",
    "print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f306bd7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "for i in [6,8,10,12,16,20,24]:\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(i,i), activation='relu', solver='adam', max_iter=500, random_state=random_seed)\n",
    "    mlp.fit(X_train, y_train.ravel())\n",
    "    predict_train = mlp.predict(X_train)\n",
    "    predict_test = mlp.predict(X_test)\n",
    "    print(\"Current nodes is: \"+ str(i))\n",
    "    print(confusion_matrix(y_test,predict_test))\n",
    "    print(classification_report(y_test,predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22098d0c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4029  412]\n",
      " [ 669  987]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.88      4441\n",
      "           1       0.71      0.60      0.65      1656\n",
      "\n",
      "    accuracy                           0.82      6097\n",
      "   macro avg       0.78      0.75      0.76      6097\n",
      "weighted avg       0.82      0.82      0.82      6097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(24,24,24), activation='relu', solver='adam', max_iter=500)\n",
    "mlp.fit(X_train, y_train.ravel())\n",
    "predict_train = mlp.predict(X_train)\n",
    "predict_test = mlp.predict(X_test)\n",
    "print(confusion_matrix(y_test,predict_test))\n",
    "print(classification_report(y_test,predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b15424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_excel('ModelingDataset-LastYear.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fafa5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=pd.read_excel('ModelingDataset-LastYear-NoNormalization.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5871534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15770, 73)\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.drop(columns=['Unnamed: 0'])\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fce04bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features and target variable\n",
    "X2 = df2.drop('employee_status', axis=1)\n",
    "y2 = df2['employee_status']\n",
    "\n",
    "# split data into train and test sets, stratifying by the target variable\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.3, random_state=random_seed, stratify=y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb5e042f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13532  1269]\n",
      " [  922    47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93     14801\n",
      "           1       0.04      0.05      0.04       969\n",
      "\n",
      "    accuracy                           0.86     15770\n",
      "   macro avg       0.49      0.48      0.48     15770\n",
      "weighted avg       0.88      0.86      0.87     15770\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tweek\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\base.py:413: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predict_test = mlp.predict(X2)\n",
    "print(confusion_matrix(y2,predict_test))\n",
    "print(classification_report(y2,predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01c9993a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tweek\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\base.py:413: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predictArray = mlp.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94730115",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['predict'] = predictArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71a37a38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8300553774000394\n",
      "0.8479992279320463\n",
      "0.8646378303330915\n",
      "0.8799830303464462\n",
      "0.8940591442294794\n",
      "0.9069015828532874\n",
      "0.9185553606853488\n",
      "0.9290735377756345\n",
      "0.9385156407225814\n",
      "0.9469461049962336\n",
      "0.9544327762229544\n",
      "0.9610455023979654\n",
      "0.9668548428059722\n",
      "0.9719309130056291\n",
      "0.9763423788569456\n",
      "0.9801556064935949\n",
      "0.9834339695726023\n",
      "0.9862373102316879\n",
      "0.9886215460586676\n",
      "0.9906384120875316\n",
      "0.9923353243961844\n"
     ]
    }
   ],
   "source": [
    "compaRatios = np.arange(.80,1.21,.02)\n",
    "compaRatiosNormal = []\n",
    "mu = df2['compa_ratio'].mean()\n",
    "sigma = df2['compa_ratio'].std()\n",
    "for i in compaRatios:\n",
    "    print(norm.cdf((i- mu) / sigma))\n",
    "    compaRatiosNormal.append(norm.cdf((i- mu) / sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06578b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281392431.6367012\n",
      "666814906.9362501\n",
      "(4790,)\n",
      "0.9388299582463464\n"
     ]
    }
   ],
   "source": [
    "#Getting numbers\n",
    "#Amount of Replacement Cost In the last year | 105095412.54429126\n",
    "print(df3['replacement_cost'][df3['predict']==1].sum())\n",
    "#Salaries paid in the last year | 812621033.0736899\n",
    "print(df3['salary'][df3['predict']==0].sum())\n",
    "print\n",
    "#Amount of voluntary churn in the last year | 1683\n",
    "print(df3['predict'][df3['predict']==1].shape)\n",
    "#Average Compa Ratio of churned employees | 0.9664218657159834\n",
    "print(df3['compa_ratio'][df3['predict']==1].sum()/df3['predict'][df3['predict']==1].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "16f10945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14224, 72)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ceb98c",
   "metadata": {
    "id": "508c1eeb"
   },
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9be4065",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38bee004",
    "outputId": "68d50cdb-0599-4fea-d349-8b2735d71717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.8187633262260128\n",
      "Precision: 0.7177865612648221\n",
      "Recall: 0.5483091787439613\n",
      "Confusion matrix:\n",
      "[[4084  357]\n",
      " [ 748  908]]\n"
     ]
    }
   ],
   "source": [
    "# calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, predict_test)\n",
    "precision = precision_score(y_test, predict_test)\n",
    "recall = recall_score(y_test, predict_test)\n",
    "\n",
    "# display evaluation metrics and confusion matrix\n",
    "print(\"\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "cm = confusion_matrix(y_test, predict_test)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2cde2dac",
   "metadata": {
    "id": "16548685"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6217048955837041\n",
      "AUC score: 0.7339609392931696\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, predict_test)\n",
    "auc_score = roc_auc_score(y_test, predict_test)\n",
    "\n",
    "print(\"F1 score:\", f1)\n",
    "print(\"AUC score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "aaf48482",
   "metadata": {
    "id": "Mdlr7SgKaGcq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
